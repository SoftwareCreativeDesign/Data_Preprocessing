# -*- coding: utf-8 -*-
"""DataSet_Check_And_Preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LaGE2-nFjIrin7EQkIeTvE46XNW1M0Gf
"""

# Mount Google Drive and install necessary libraries

from google.colab import drive
drive.mount('/content/drive')

# Import necessary libraries

import pandas as pd
import numpy as np
import re
from urllib.parse import urlparse

# Load the data
# Update this path to point to your new CSV file with 'URL' and 'Nor/Mal' columns.
# Load the data with specified encoding

data = pd.read_csv('/content/drive/MyDrive/DaeguCatholicUniversity_Software_Creative_Design_01/Malicious_URL_Data.csv',
                   usecols=['URL', 'Nor/Mal'], encoding='ISO-8859-1')  # OR Using encoding='cp949'

# Display first few rows to check data structure

data.head()

# Feature Engineering Functions

# 1. Check if IP address is used in the URL

def check_ip(url):
    return 1 if re.match(r'http[s]?://\d+\.\d+\.\d+\.\d+', url) else 0

# 2. Check if URL length is greater than a threshold (e.g., 54 characters)

def long_url(url):
    return len(url)

# 3. Check if URL uses shortening services

def is_shortened(url):
    shortening_services = r"bit\.ly|tinyurl\.com|goo\.gl|shorte\.st|...|adf\.ly|t\.co|ow\.ly|tiny\.cc|is\.gd|...|clk\.im"
    return 1 if re.search(shortening_services, url) else 0

# 4. Check if '@' symbol is present in the URL

def has_at_symbol(url):
    return 1 if "@" in url else 0

# 5. Check for "//" after protocol in the URL path (redirect)

def has_double_slash(url):
    path = urlparse(url).path
    return 1 if '//' in path else 0

# 6. Check for dash in domain part of URL

def has_dash(url):
    domain = urlparse(url).netloc
    return 1 if '-' in domain else 0

# 7. Count subdomains

def count_subdomains(url):
    domain = urlparse(url).netloc
    return len(domain.split('.')) - 2

# 8. Check if HTTPS is used

def is_https(url):
    return 1 if urlparse(url).scheme == 'https' else 0

# 9. Check domain registration length (Mock Feature: Replace this with real WHOIS data if available)

def domain_registration_length(url):
    return np.random.randint(1, 5) # 1 for short-term, 5 for long-term

# 10. Check for existence of favicon (Mock Feature: Replace with real check if favicon URL is different from domain)

def has_favicon(url):
    return np.random.choice([0, 1])

# 11. Check for non-standard ports (Mock Feature: Replace with actual port checks if possible)

def has_non_standard_port(url):
    return np.random.choice([0, 1])

# Apply feature extraction

data['has_ip'] = data['URL'].apply(check_ip)
data['long_url'] = data['URL'].apply(long_url)
data['is_shortened'] = data['URL'].apply(is_shortened)
data['has_at_symbol'] = data['URL'].apply(has_at_symbol)
data['has_double_slash'] = data['URL'].apply(has_double_slash)
data['has_dash'] = data['URL'].apply(has_dash)
data['subdomain_count'] = data['URL'].apply(count_subdomains)
data['is_https'] = data['URL'].apply(is_https)
data['domain_reg_length'] = data['URL'].apply(domain_registration_length)
data['has_favicon'] = data['URL'].apply(has_favicon)
data['non_standard_port'] = data['URL'].apply(has_non_standard_port)

# Save the processed data to a new CSV file
# Specify the output file path here

output_path = '/content/drive/MyDrive/DaeguCatholicUniversity_Software_Creative_Design_01//Processed_URL_Data.csv'
data.to_csv(output_path, index=False)

print(f"Processed data saved to {output_path}")